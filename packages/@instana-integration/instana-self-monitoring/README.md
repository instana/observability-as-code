# @instana-integration/instana-self-monitoring

This integration package provides dashboards and event monitoring to support self-observability for Instanaâ€™s internal systems.

## Dashboards

Below are the dashboards that are currently supported by this integration package.

| Dashboard Title                                        | Description                                                                            |
|--------------------------------------------------------|----------------------------------------------------------------------------------------|
| OAC - Self Monitoring: profile related pipelines       | Monitors health and activity of pipelines related to profiling functionality.          |
| OAC - Self Monitoring: scale high perf metrics down    | Tracks how high-performance metric services behave during scale-down.                  |
| OAC - Self Monitoring: scale high perf processor down  | Observes performance and resource usage when scaling down high-perf processors.        |
| OAC - Self Monitoring: scale high perf spans down      | Monitors system behavior when scaling down span-handling components in high-perf mode. |
| OAC - Self Monitoring: scale high perf ui profile down | Tracks UI profiling components under high-performance scale-down scenarios.            |
| OAC - Self Monitoring: scale metrics profile down      | Provides insights into metric profiling as related components are scaled down.         |
| OAC - Self Monitoring: scale processor down            | Visualizes resource and performance metrics during standard processor scale-down.      |
| OAC - Self Monitoring: scale spans profile down        | Monitors how span profiles react to downscaling actions.                               |
| OAC - Self Monitoring: scale ui profile down           | Evaluates the impact of scaling down UI profiling modules.                             |
| OAC - Self Monitoring: Usage k8s components            | Displays usage and performance of Kubernetes components across the cluster.            |


## Metrics

### Semantic Conventions

Below are the runtime metrics that are currently supported by this integration package.

[None]

### Resource Attributes

Below are the resource attributes that are currently supported by this integration package.

[None]

## Events

Below are the events that are currently supported by this integration package.

Note: In each event definition, conditionValue represents a threshold used to trigger the event and is provided as a default or reference value. Please adjust this value based on your specific environment.

| Event Name                                                                      | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
|---------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [SRESLO] Acceptor HTTP job queue is filling up [TF]                             | The HTTP threads of this instance do not seem to be able to handle the number of incoming requests causing job queue to fill up. This may eventually result in rejected HTTP requests and Acceptor responding with HTTP status code ENHANCE_YOUR_CALM. Potential causes for this are the respective Acceptor instance receiving too much traffic or problems with the Kafka cluster.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| [SRESLO] Acceptor spans_error_rate rate is too high [TF]                        | This indicates problems with either too high load on the individual Acceptor node or problems with the Kafka cluster in this region and leads to gaps in metrics and calls for customers.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| appdata-processor - Failed to heal online infra graph [TF]                      | https://www.notion.so/instana/appdata-processor-Failure-to-heal-online-infra-graph-8daaf2073f0143f790607e78abb4a663                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| appdata-processor - Failed to heal online snapshots [TF]                        | https://www.notion.so/instana/appdata-processor-failed-to-heal-online-snapshots-c36dd6b9f2ec4a26ac4717d92c1911ba                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| [SRETUSLO] appdata-processor is dropping > 25% spans [TF]                       | This can lead to missing traces and results in wrong call counts. Possible reasons are an increase in ingress for the customer or that the component is running on an overloaded worker. If the traffic is reasonable (e.g. not spikes through load tests) consider scaling the spans profile for this TU up or scale out appdata-processor. See also https://www.notion.so/instana/appdata-processor-Spans-are-being-dropped-173977adc97d4e1b9b08bbbc7e56b8ec                                                                                                                                                                                                                                                                                                                                                                                                      |
| appdata-processor - service explosion [TF]                                      | https://www.notion.so/instana/appdata-processor-Service-explosion-6794827fd9484a1cb557c06c7f7ce969                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| [SRESLO] Butler HTTP thread pool is very busy [TF]                              | The HTTP threads of this instance do not seem to be able to handle the number of incoming requests causing job queue to fill up. This may eventually result in OOPS page in web-UI. Potential cause for this is butler produce exceeded db access(queries) to PostgreSQL(butler) in rainbow. You might need to check the PostgreSQL in rainbow.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| [SRESLO] Butler http thread is very busy [TF]                                   | The HTTP threads of this instance do not seem to be able to handle the number of incoming requests causing job queue to fill up. This may eventually result in OOPS page in web-UI. Potential cause for this is butler produce exceeded db access(queries) to PostgreSQL(butler) in rainbow. You might need to check the PostgreSQL in rainbow.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| [SREInfraSLO] metrics-cassandra node unreachable for > 5m [TF]                  | A node in the metrics-cassandra cluster is not reachable since 5 minutes. This usually indicates overall load problems in the cluster or a issue with the underlying cloud VM. Try restarting the process and investigate overall cluster load afterwards.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| [SREInfraSLO] profiles-cassandra node unreachable for > 5m [TF]                 | A node in the profiles-cassandra cluster is not reachable since 5 minutes. This usually indicates overall load problems in the cluster or a issue with the underlying cloud VM. Try restarting the process and investigate overall cluster load afterwards.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| [SREInfraSLO] spans-cassandra node unreachable for > 5m [TF]                    | A node in the spans-cassandra cluster is not reachable since 5 minutes. This usually indicates overall load problems in the cluster or a issue with the underlying cloud VM. Try restarting the process and investigate overall cluster load afterwards.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| [SREInfraSLO] state-cassandra node unreachable for > 5m [TF]                    | A node in the state-cassandra cluster is not reachable since 5 minutes. This usually indicates overall load problems in the cluster or a issue with the underlying cloud VM. Try restarting the process and investigate overall cluster load afterwards.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| [SREInfraSLO] state-cassandra write latency greater than 1 minute [TF]          | A node in the state-cassandra cluster is having very high write latency. This indicates a major issue which will affect dependent components like tag-processor. Try restarting the node.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| ClickHouse - AP - Too many parts in one of the partitions of the XXX table [TF] | https://www.notion.so/instana/ClickHouse-Too-many-parts-in-one-of-the-partitions-of-the-XXX-table-01b7add852d74a499ed0216115ed8804. ##Note: The 'conditionValue' (currently set to 830) is a sample reference for the variable 'active_parts_threshold'. Please adjust this value based on your specific environment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| ClickHouse - AP - Too Many inserts are rejected [TF]                            | The data is getting lost. Caused by 'Too many parts' exception due to high number of active data parts for partition. https://www.notion.so/instana/ClickHouse-Too-many-parts-in-one-of-the-partitions-of-the-XXX-table-01b7add852d74a499ed0216115ed8804. ##Note: The 'conditionValue' (currently set to 13500) is a sample reference for the variable 'clickhouse_rejected_inserts_threshold'. Please adjust this value based on your specific environment.                                                                                                                                                                                                                                                                                                                                                                                                        |
| ClickHouse - AP - Too many simultaneous queries [TF]                            | https://www.notion.so/instana/ClickHouse-Too-many-simultaneous-queries-8adf725a6c4c49088eee518b1af6880e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| [SREInfraSLO] ClickHouse - Distribution queue is filling up [TF]                | https://www.notion.so/instana/Clickhouse-Distribution-queue-is-filling-up-9453591a12df4a5bb9e27                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| [SREInfraSLO] ClickHouse is not running on the host. [TF]                       | The ClickHouse process is not running on the underlying host. This can be caused by issues related to the replication process. Please check ClickHouse logs for any recurring exceptions / errors and if none are present you [start the ClickHouse recovery process](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication/#recovery-after-failures).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| [SREInfraSLO] Clickhouse disk fs./dev/nvme1n1.used more than 85% full. [TF]     | This is a critical alert and if no action, the Clickhouse Cluster can go down. A disk attached to a Clickhouse server at fs./dev/nvme1n1.used is greater than 85%. This indicates that the disk is going to reach its maximum capacity. When triggered, please check the Clickhouse cluster capacity and coordinate with the rest of the team to decide to increase the disk or add more shards.                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| [SREInfraSLO] Clickhouse disk fs./dev/nvme2n1.used more than 85% full. [TF]     | This is a critical alert and if no action, the Clickhouse Cluster can go down. A disk attached to a Clickhouse server at fs./dev/nvme2n1.used is greater than 85%. This indicates that the disk is going to reach its maximum capacity. When triggered, please check the Clickhouse cluster capacity and coordinate with the rest of the team to decide to increase the disk or add more shards.                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| [SREInfraSLO] Clickhouse disk fs./dev/nvme3n1.used more than 85% full. [TF]     | This is a critical alert and if no action, the Clickhouse Cluster can go down. A disk attached to a Clickhouse server at fs./dev/nvme3n1.used is greater than 85%. This indicates that the disk is going to reach its maximum capacity. When triggered, please check the Clickhouse cluster capacity and coordinate with the rest of the team to decide to increase the disk or add more shards.                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| [SREInfraSLO] Clickhouse disk fs./dev/sdb.used more than 85% full. [TF]         | This is a critical alert and if no action, the Clickhouse Cluster can go down. A disk attached to a Clickhouse server at fs./dev/sdb.used is greater than 85%. This indicates that the disk is going to reach its maximum capacity. When triggered, please check the Clickhouse cluster capacity and coordinate with the rest of the team to decide to increase the disk or add more shards.                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| [SREInfraSLO] Clickhouse disk fs./dev/sdc.used more than 85% full. [TF]         | This is a critical alert and if no action, the Clickhouse Cluster can go down. A disk attached to a Clickhouse server at fs./dev/sdc.used is greater than 85%. This indicates that the disk is going to reach its maximum capacity. When triggered, please check the Clickhouse cluster capacity and coordinate with the rest of the team to decide to increase the disk or add more shards.                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| ClickHouse - Queries failing due to memory limit exceeded [TF]                  | https://www.notion.so/instana/ClickHouse-Queries-failing-due-to-memory-limit-exceeded-8f2a3e6daea2438eb97289c0892459f2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| [SREInfraSLO] ClickHouse replica is in read-only mode for > 1m [TF]             | This can occur if there is an unknown error during (re)initialization of sessions with Zookeeper, or during session reinitialization in ZK itself. Run the following command on the alerting ClickHouse node to verify that the replica is still in read-only mode. Initiate Clickhouse client first via root user using the following command: `clickhouse-client` Once client has started, use: `SELECT * FROM system.replicas WHERE is_readonly = 1 LIMIT 10;` If nothing points to more severe problems, try to **restart the ClickHouse node that contains the read-only replica.**                                                                                                                                                                                                                                                                            |
| ClickHouse - Inserts are rejected [TF]                                          | The data is getting lost. Caused by 'Too many parts' exception due to high number of active data parts for partition. https://www.notion.so/instana/ClickHouse-Too-many-parts-in-one-of-the-partitions-of-the-XXX-table-01b7add852d74a499ed0216115ed8804                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| ClickHouse - Replica Max Insert Queue Filling Up [TF]                           | There is an increase of files that are stockpiling at the Clickhouse level. The associated graph in SRE:Clickhouse-AP should highlight whether this is a collection of CH nodes, or if a single CH node is handling most of the files that are piling up. Additionally, Appdata-Writer errors may be present and may need investigation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| ClickHouse - Replica Max Insert Queue Filling Up [TF]                           | There is an increase in insertions for a Clickhouse server, or multiple servers. There may also be an increase in Distributed Files to Insert. Additional items to check are Appdata-Writer Clickhouse Error Rates. For a development perspective of this error: https://www.notion.so/instana/ClickHouse-Replication-queue-is-filling-up-405a445e25be4316add4cca943bf9d20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| ClickHouse - Replication queue is filling up [TF]                               | https://www.notion.so/instana/ClickHouse-Replication-queue-filling-up-405a445e25be4316add4cca943bf9d20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| [SREInfraSLO] Zookeeper is not running for Clickhouse. [TF]                     | The Zookeeper process is not running on the underlying host. This will prevent Clickhouse from functioning untill it is restored.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| [SREInfraSLO] Elasticsearch-NG has high number of unassigned shards [TF]        | Number of unassigned shards are increasing and is above 5. This can indicate underlying resource issue. There may not be enough elastic data nodes to assign the new shards.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| [SREInfraSLO] Elasticsearch-NG has high number of unassigned shards [TF]        | Number of unassigned shards are increasing and is above 5. This can indicate underlying resource issue. There may not be enough elastic data nodes to assign the new shards.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| [EXPTUSLO] High ES Client Search Error Rate [TF]                                | Searches from this component to ElasticSearch are failing at a high rate. ## Consequences - If this is a user facing component, users may be seeing errors on the UI - If this is a processing component, processing may not be proceeding as fast as necessary, or data may be out of sync, leading to incorrect information being shown on the UI ## Remediation - Check that ElasticSearch (NG) is healthy in this region. - Check that this component is able to connect to Elasticsearch - If the error rate is very high, and ES is otherwise healthy, a component restart may resolve the issue                                                                                                                                                                                                                                                              |
| [SREInfraSLO] elasticsearch data disk utilized > 85% [TF]                       | The elasticsearch disk usage for data is greater than 85%. This indicates the disk is getting closer to max capacity. This may require cleanup of disk space. If not addressed in time, it may cause the node to stop working and possibly leave the cluster.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| [SREInfraSLO] elasticsearch data disk utilized > 85% [TF]                       | The elasticsearch disk usage for data is greater than 85%. This indicates the disk is getting closer to max capacity. This may require cleanup of disk space. If not addressed in time, it may cause the node to stop working and possibly leave the cluster.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| [SREInfraSLO] elasticsearch data disk utilized > 85% [TF]                       | The elasticsearch disk usage for data is greater than 85%. This indicate the disk is getting closer to max capacity. This may require to cleanup the disk space. If this is not fixed on time, it may cause the node stop working and possible leave the cluster.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| [SREInfraSLO] elasticsearch data disk utilized > 85% [TF]                       | The elasticsearch disk usage for data is greater than 85%. This indicates the disk is getting closer to max capacity. This may require cleanup of disk space. If not addressed in time, it may cause the node to stop working and possibly leave the cluster.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| [SREInfraSLO] Elasticsearch is not running on the host. [TF]                    | The Elasticsearch process is not running on the underlying host. Please check Elasticsearch logs for any recurring exceptions / errors and if none are present restart the process                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| [SREInfraSLO] elasticsearch is in red status [TF]                               | Elasticsearch is in red status. This indicates that at least one primary shard and all of its replicas are not allocated to a node, meaning that some data is unavailable. Please check this issue as it will impact read/write operations.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| [SREInfraSLO] elasticsearch is in red status [TF]                               | Elasticsearch is in red status. This indicates that at least one primary shard and all of its replicas are not allocated to a node, meaning that some data is unavailable. Please check this issue as it will impact read/write operations.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| [SREInfraSLO] elasticsearch root disk utilized > 85% [TF]                       | The elasticsearch disk usage for root is greater than 85%. This indicate the disk is getting closer to max capacity around 20G. This may require to cleanup the disk space. If this is not fixed on time, it may cause the node stop working and possible leave the cluster.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| [DEVTUSLO] Instana filler pod CPU usage to CPU requests > 200%  [TF]            | This Instana filler pod's CPU usage is over twice the CPU requests for more than 1h. This either indicates that the pod has too little requests and should be resized or that there is a problem with an upstream component (e.g. spike in usage on a specific pod).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| [SRETUSLO] filler is dropping > 10% Cassandra rollups [TF]                      | Rollups are being dropped before being written to Metrics Cassandra. This can be caused by too high load on the worker node or high processing pressure on the filler.  ## Consequences This means there will be gaps in historical data for metrics.  ## Remediation - Increase the TU toggle `config.rollup.cassandra.downstream.threads` (Default is 1. Shouldn't be more than 4).  To get more context: [ðŸ”µ](https://blue-instanaops.instana.io/#/customDashboards/view;dashboardId=R-BySNacQiGmIwSXpvAVqQ) [ðŸŸ¢](https://green-instanaops.instana.io/#/customDashboards/view;dashboardId=eppNF3E5SIGcUYw1-DCEUQ) [ðŸ”´](https://red-instanaops.instana.io/#/customDashboards/view;dashboardId=jDWDz6P_RU-5zwQ37IioFw) [ðŸŸ ](https://orange-instanaops.instana.io/#/customDashboards/view;dashboardId=Ml9uzhNNRqu5_QJQ3AZoYQ)                                                                                                                                                                                                                                                                                                                    |
| [SRETUSLO] filler writes to Cassandra are failing [TF]                          | There is a high error rate when trying to write rollups to Cassandra. This can be caused by bugs (e.g. unsupported rollups used) or *a lot more likely* a broken connection between `filler` and Metrics Cassandra or general Metrics Cassandra problems.  ## Consequences Alerting and live metrics viewing in the UI will be affected.  ## Remediation - Check whether Cassandra is accessible from `filler`. - Otherwise report to team Infrastructure.  To get more context: [ðŸ”µ](https://blue-instanaops.instana.io/#/customDashboards/view;dashboardId=R-BySNacQiGmIwSXpvAVqQ) [ðŸŸ¢](https://green-instanaops.instana.io/#/customDashboards/view;dashboardId=eppNF3E5SIGcUYw1-DCEUQ) [ðŸ”´](https://red-instanaops.instana.io/#/customDashboards/view;dashboardId=jDWDz6P_RU-5zwQ37IioFw) [ðŸŸ ](https://orange-instanaops.instana.io/#/customDashboards/view;dashboardId=Ml9uzhNNRqu5_QJQ3AZoYQ)                                                                                                                                                                                                                                                                                                                                           |
| [EXPTUSLO] filler is failing to publish group member data [TF]                  | Sending group members to the group processing instance is failing at a high rate. This probably indicates a serious issue with the kafka producer.  ## Consequences Group entities will not be updated  ## Remediation - Restart `filler`  To get more context: [ðŸ”µ](https://blue-instanaops.instana.io/#/customDashboards/view;dashboardId=5ilU_JJoTuWHKd9uz1PKXQ) [ðŸŸ¢](https://green-instanaops.instana.io/#/customDashboards/view;dashboardId=WnDP6tKCTTOKCBIeR9Dx1w) [ðŸ”´](https://red-instanaops.instana.io/#/customDashboards/view;dashboardId=1eqIS6ThSrud09h-fZzLRA) [ðŸŸ ](https://orange-instanaops.instana.io/#/customDashboards/view;dashboardId=n1ruCLQvQhm--a1IbkXxyw)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| [EXPTUSLO] group member metrics data are delayed [TF]                           | Group member aggregation is being processed with a long delay - this means group member metrics are very delayed This is a sign that GroupMemberDataSource in Filler cannot keep up with the number data messages are written to the group_members topic.  ## Consequences Group member metrics and entities might be dropped and may display an incorrect data  ## Remediation - scale saas_instanaops_*_group_members topic to multiple partitions to allow multiple Filler instances to consume the group members topic  To get more context: [ðŸ”µ](https://blue-instanaops.instana.io/#/customDashboards/view;dashboardId=5ilU_JJoTuWHKd9uz1PKXQ) [ðŸŸ¢](https://green-instanaops.instana.io/#/customDashboards/view;dashboardId=WnDP6tKCTTOKCBIeR9Dx1w) [ðŸ”´](https://red-instanaops.instana.io/#/customDashboards/view;dashboardId=1eqIS6ThSrud09h-fZzLRA) [ðŸŸ ](https://orange-instanaops.instana.io/#/customDashboards/view;dashboardId=n1ruCLQvQhm--a1IbkXxyw)                                                                                                                                                                                                                                                                           |
| [SRETUSLO] filler is dropping info events [TF]                                  | Events are being dropped before being published to the events kafka topic. ## Consequences This will cause alerts not to be triggered. ## Remediation - If you see this error for several fillers in one region this indicates problems with the Kafka cluster. - Check the logs to see if any unexpected errors have occurred. - If the metric `com.instana.filler.raw-messages.processed` has dropped to zero, no messages are flowing and `filler`.   - Take a heap dump and inform team Infrastructure   - Restart `filler` To get more context: [ðŸ”µ](https://blue-instanaops.instana.io/#/customDashboards/view;dashboardId=C3eN96Z8RyOJ7ycpgHkK5Q) [ðŸŸ¢](https://green-instanaops.instana.io/#/customDashboards/view;dashboardId=NXPy2nCSSeOsiKS2Rd6O-g) [ðŸ”´](https://red-instanaops.instana.io/#/customDashboards/view;dashboardId=XvhTJitbRAmM3hhMOOKPbQ) [ðŸŸ ](https://orange-instanaops.instana.io/#/customDashboards/view;dashboardId=4YntvQ3eSsaJ-BUXgoB2Wg)                                                                                                                                                                                                                                                                       |
| [SRETUSLO] filler is dropping > 10% Kafka rollups [TF]                          | Rollups are being dropped before being written to Kafka. This can be caused by too high load on the worker node or high processing pressure on the filler.  ## Consequences  Alerting and live metrics viewing in the UI will be affected.  ## Remediation  - Increasing the TU toggle `config.rollup.kafka.downstream.threads` (Default is 1. Shouldn't be more than 4).  To get more context: [ðŸ”µ](https://blue-instanaops.instana.io/#/customDashboards/view;dashboardId=R-BySNacQiGmIwSXpvAVqQ) [ðŸŸ¢](https://green-instanaops.instana.io/#/customDashboards/view;dashboardId=eppNF3E5SIGcUYw1-DCEUQ) [ðŸ”´](https://red-instanaops.instana.io/#/customDashboards/view;dashboardId=jDWDz6P_RU-5zwQ37IioFw) [ðŸŸ ](https://orange-instanaops.instana.io/#/customDashboards/view;dashboardId=Ml9uzhNNRqu5_QJQ3AZoYQ)                                                                                                                                                                                                                                                                                                                                                                                                                            |
| [SRETUSLO] filler raw message lag > 5 seconds [TF]                              | Filler raw message processing is lagging. The lag has been greater than five seconds for the last fifteen minutes.  ## Consequences We are ingesting both tags and metrics more slowly than we should. Data will not become available in Instana in a timely fashion or might be dropped.  ## Remediation Steps to take _until remediation_ - Upsize the metrics profile for the TU. - Move this filler to the highperf nodepool. - Move off other high-load components running on the same worker. - Scale out the filler.                                                                                                                                                                                                                                                                                                                                         |
| [EXPTUSLO] filler outdated snapshots [TF]                                       | There is a difference in the snapshot cache and the snapshot state from ElasticSearch.  ## Consequences If this condition persists, it could lead to outdated entities.  ## Remediation - Check ElasticSearch cluster load. - If this is specific to this tenant, possibly there are too many online snapshots.  To get more context: [ðŸ”µ](https://blue-instanaops.instana.io/#/customDashboards/view;dashboardId=WgHDS_DRRSWjVYOzKoojeg) [ðŸŸ¢](https://green-instanaops.instana.io/#/customDashboards/view;dashboardId=gjymdolcRCmhsSBDYa9tEw) [ðŸ”´](https://red-instanaops.instana.io/#/customDashboards/view;dashboardId=d4cBzHW5Se-zzmtgQ9SLsw) [ðŸŸ ](https://orange-instanaops.instana.io/#/customDashboards/view;dashboardId=OYEqU6yTTFmsdkhnuPNidQ)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| [SRETUSLO] filler is dropping raw events [TF]                                   | Raw events are being dropped before being published to the raw_events kafka topic. ## Consequences This will cause alerts not to be triggered. ## Remediation - If you see this error for several fillers in one region this indicates problems with the Kafka cluster. - Check the logs to see if any unexpected errors have occurred. - If the metric `com.instana.filler.raw-messages.processed` has dropped to zero, no messages are flowing and `filler`.   - Take a heap dump and inform team Infrastructure   - Restart `filler` To get more context: [ðŸ”µ](https://blue-instanaops.instana.io/#/customDashboards/view;dashboardId=C3eN96Z8RyOJ7ycpgHkK5Q) [ðŸŸ¢](https://green-instanaops.instana.io/#/customDashboards/view;dashboardId=NXPy2nCSSeOsiKS2Rd6O-g) [ðŸ”´](https://red-instanaops.instana.io/#/customDashboards/view;dashboardId=XvhTJitbRAmM3hhMOOKPbQ) [ðŸŸ ](https://orange-instanaops.instana.io/#/customDashboards/view;dashboardId=4YntvQ3eSsaJ-BUXgoB2Wg)                                                                                                                                                                                                                                                               |
| [SRETUSLO] filler is dropping > 15% raw messages [TF]                           | Raw messages, i.e. messages coming from agents, are being dropped. This is a sign that `filler` is running with too little resources or on a over-utilized host.  ## Consequences  This will cause gaps in metrics dashboards, broken alerting and permanent online/offline events.  ## Remediation  Steps to take _until remediation_  - Upsize the metrics profile for the TU. - Move this filler to the highperf nodepool. - Move off other high-load components running on the same worker. - Scale out the filler.  If all the above have been done, then the filler is likely hitting limits in the snapshot processing pipeline. The infra team is working on improving limits and performance for the snapshot processing pipeline, but please inform team infra for further investigations.  To get more context: [ðŸ”µ](https://blue-instanaops.instana.io/#/customDashboards/view;dashboardId=C3eN96Z8RyOJ7ycpgHkK5Q) [ðŸŸ¢](https://green-instanaops.instana.io/#/customDashboards/view;dashboardId=NXPy2nCSSeOsiKS2Rd6O-g) [ðŸ”´](https://red-instanaops.instana.io/#/customDashboards/view;dashboardId=XvhTJitbRAmM3hhMOOKPbQ) [ðŸŸ ](https://orange-instanaops.instana.io/#/customDashboards/view;dashboardId=4YntvQ3eSsaJ-BUXgoB2Wg) |
| [EXPTUSLO] filler is failing to publish raw metric data [TF]                    | Sending raw metrics to kafka (for beeinstana ingestion) is failing at a high rate. This probably indicates a serious issue with the kafka producer.  ## Consequences Metrics will be lost.  ## Remediation - Restart `filler`  To get more context: [ðŸ”µ](https://blue-instanaops.instana.io/#/customDashboards/view;dashboardId=R-BySNacQiGmIwSXpvAVqQ) [ðŸŸ¢](https://green-instanaops.instana.io/#/customDashboards/view;dashboardId=eppNF3E5SIGcUYw1-DCEUQ) [ðŸ”´](https://red-instanaops.instana.io/#/customDashboards/view;dashboardId=jDWDz6P_RU-5zwQ37IioFw) [ðŸŸ ](https://orange-instanaops.instana.io/#/customDashboards/view;dashboardId=Ml9uzhNNRqu5_QJQ3AZoYQ)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| [SRETUSLO] filler is dropping snapshot changes [TF]                             | Snapshot changes are being dropped before being written to ElasticSearch. This is a sign that `filler` is running with too little resources or on a over-utilized host. ## Consequences All reporting entities in this TU will go offline and there will effectively be no infrastructure monitoring. ## Remediation - Restart `filler` To get more context: [ðŸ”µ](https://blue-instanaops.instana.io/#/customDashboards/view;dashboardId=C3eN96Z8RyOJ7ycpgHkK5Q) [ðŸŸ¢](https://green-instanaops.instana.io/#/customDashboards/view;dashboardId=NXPy2nCSSeOsiKS2Rd6O-g) [ðŸ”´](https://red-instanaops.instana.io/#/customDashboards/view;dashboardId=XvhTJitbRAmM3hhMOOKPbQ) [ðŸŸ ](https://orange-instanaops.instana.io/#/customDashboards/view;dashboardId=4YntvQ3eSsaJ-BUXgoB2Wg)                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| [EXPTUSLO] filler is not processing any snapshot data [TF]                      | Filler is not processing any entity heartbeats, which means it is not processing any snapshots.  ## Consequences All reporting entities in this TU will go offline and there will effectively be no infrastructure monitoring.  ## Remediation - Restart `filler`  To get more context: [ðŸ”µ](https://blue-instanaops.instana.io/#/customDashboards/view;dashboardId=R-BySNacQiGmIwSXpvAVqQ) [ðŸŸ¢](https://green-instanaops.instana.io/#/customDashboards/view;dashboardId=eppNF3E5SIGcUYw1-DCEUQ) [ðŸ”´](https://red-instanaops.instana.io/#/customDashboards/view;dashboardId=jDWDz6P_RU-5zwQ37IioFw) [ðŸŸ ](https://orange-instanaops.instana.io/#/customDashboards/view;dashboardId=Ml9uzhNNRqu5_QJQ3AZoYQ)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| [EXPTUSLO] filler stopped synchronizing ES search index [TF]                    | Check for unexpired snapshots has not successfully run.  ## Consequences If this condition persists, it could lead to DFQ searches not returning correct results  ## Remediation - Check ElasticSearch cluster load. - Check `filler` logs.  To get more context: [ðŸ”µ](https://blue-instanaops.instana.io/#/customDashboards/view;dashboardId=WgHDS_DRRSWjVYOzKoojeg) [ðŸŸ¢](https://green-instanaops.instana.io/#/customDashboards/view;dashboardId=gjymdolcRCmhsSBDYa9tEw) [ðŸ”´](https://red-instanaops.instana.io/#/customDashboards/view;dashboardId=d4cBzHW5Se-zzmtgQ9SLsw) [ðŸŸ ](https://orange-instanaops.instana.io/#/customDashboards/view;dashboardId=OYEqU6yTTFmsdkhnuPNidQ)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| [SRETUSLO] filler stopped fetching raw messages [TF]                            | Metrics may not have been flowing for several minutes.  ## Consequences If this condition persists, it will result in loss of metrics.  ## Remediation - If you see this error for several fillers in one region this indicates problems with the Kafka cluster. - Check the logs to see if any unexpected errors have occurred. - If the metric `com.instana.filler.raw-messages.processed` has dropped to zero, no messages are flowing in `filler`.   - Take a heap dump and inform team Infrastructure   - Restart `filler`  To get more context: [ðŸ”µ](https://blue-instanaops.instana.io/#/customDashboards/view;dashboardId=C3eN96Z8RyOJ7ycpgHkK5Q) [ðŸŸ¢](https://green-instanaops.instana.io/#/customDashboards/view;dashboardId=NXPy2nCSSeOsiKS2Rd6O-g) [ðŸ”´](https://red-instanaops.instana.io/#/customDashboards/view;dashboardId=XvhTJitbRAmM3hhMOOKPbQ) [ðŸŸ ](https://orange-instanaops.instana.io/#/customDashboards/view;dashboardId=4YntvQ3eSsaJ-BUXgoB2Wg)                                                                                                                                                                                                                                                                      |
| [SRESLO] Synthetics Datacenter Activation Errors [TF]                           | This indicates problems in the Synthetics Datacenter Activation / Managed Location Deployment process.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| [SREInfraSLO] Kafka node is showing high I/O wait times [TF]                    | A Kafka node is showing high wait times for a longer period of time. This can indicate underlying hardware issues (in case of local SSDS), a general load issue with the Kafka cluster in that region or a load increase for a certain customers whos partitions are located on that node. In case of hardware failures, replace the node in questions. Otherwise look into balancing topics acros the cluster to spread the load.                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| [SREInfraSLO] Kafka node is approaching max open files [TF]                     | A Kafka node has too many open files. This can result in a crash or restart of one or more kafka nodes in the cluster. Collect logs, check the kafka service on the host is running, and restart if necessary.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| [SREInfraSLO] Kafka node has under replicated topic partitions [TF]             | There are under replicated topic partitions on a Kafka node for a longer period of time. This can indicate one or more Kafka nodes are down or the Kafka cluster is overloaded and followers can't catch up from leaders. In case of hardware failures, replace the node in questions. If it's cluster overloaded, consider to expand Kafka cluster. If the node is overloaded(significant topics on the node) instead of the whole cluster, try to balance the topics across the cluster.                                                                                                                                                                                                                                                                                                                                                                          |
| [DevSLO] Incoming logs are lagging [TF]                                         | LHP is significantly lagging to consume from `enriched_logs` topic. This can result in `false positives` (e.g. wrong unusual low number of logs detected) or `false negatives` (no detection) in Smart-Alerts. Even when the dropping metrics indicate a low dropping rate of logs, depending on the lag and Smart Alert rule, this can mean that there is hidden dropping, either by hitting the Kafka retention limits, or by the fact that the logs are consumed too late to be considered. **Action Item:** Check the internal dashboard for dropping and lag of logs, and whether the bottleneck is the processing capacity of LHP, or originating upstream.                                                                                                                                                                                                   |
| [DevSLO] synthetics-health-processor is dropping test results [TF]              | `synthetics-health-processor is dropping more than 1% of incoming test results.`Action Item:`Check the internal dashboard for dropping, and whether this is just due to a general increase in incoming test results:` - if `yes`: Ask SRE to scale-out - if `no`: Investigate whether a performance problem or bug was introduced                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| [DevSLO] synthetics-health-processor is dropping test results [TF]              | `synthetics-health-processor is dropping more than 10% of incoming test results.` `Action Item:`Check the internal dashboard for dropping, and whether this is just due to a general increase in incoming test results: - if `yes`: Ask SRE to scale-out - if `no`: Investigate whether a performance problem or bug was introduced                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| [DevSLO] synthetics-health-processor is not receiving test results [TF]         | `synthetics-health-processor is not receiving test results.` `Action Item:` Most likely there is a problem in synthetics-health-processor or in the components upstream: - As a quick fix, ask SRE to restart - Investigate whether a problem or bug was introduced in synthetics-health-processor or upstream components                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| [SLO] Writes of health events to Kafka are failing [TF]                         | Failed writes of health events to Kafka means loss of events in the events product area and loss of alerts send out to the alert channels. Check the logs and the network between this node and the Kafka nodes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| [SRESLO] Synthetics Acceptor Addon License Errors [TF]                          | This indicates problems with the Synthetics Acceptor receiving data from a Managed location for a TU that does not have a synthetic addon license purchased.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| [SRESLO] Synthetics Acceptor External Storage Write Errors [TF]                 | This indicates problems with the Synthetics Acceptor inserting data to the External Storage.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| [SRESLO] Synthetics Reader ClickHouse Queued Calls [TF]                         | This indicates problems with the Synthetics Reader queuing calls to retrieve data from ClickHouse. ## Consequences This will cause Synthetics Tests and Locations pages to timeout without showing any data. ## Remediation - If the metric `metrics.gauges.clickHouse.clustered.queuedCalls` is high, the execution time of all calls will increase causing timeouts:  - Take a thread dump of the `synthetics-reader` pods and inform team Synthetics  - Restart `synthetics-reader`                                                                                                                                                                                                                                                                                                                                                                              |
| [SRESLO] Synthetics Reader ClickHouse Select Failures [TF]                      | This indicates problems with the Synthetics Reader retrieving data from ClickHouse.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| [DevSLO] - Synthetics Reader External Storage Read Errors [TF]                  | This indicates problems with the Synthetics Reader retrieving data from the External Storage.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| [SRESLO] Synthetics Writer ClickHouse Insert Failures [TF]                      | This indicates problems with the Synthetics Writer inserting data in ClickHouse.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| [SRESLO] tag-processor is not cleaning up tag sets [TF]                         | Tag processor is is not running the cleanup of tag sets. ## Consequences Changes which have been dropped will not be fixed and updates to tag sets will not be reflected. This could cause some entities to not show up in searches and smart alerts based on these tags become unreliable and have false alerts. ## Remediation Steps to take _until remediation_ - Verify that there are no issues with Cassandra and Kafka. - Consider a rolling restart of pods - Inform engineering                                                                                                                                                                                                                                                                                                                                                                            |
| [SRESLO] tag-processor is dropping snapshot updates [TF]                        | Tag processor is dropping records when reading the `snapshots` topic to reduce latency. This means tag-processor is having trouble keeping up with the number of snapshot updates for some TUs. ## Consequences As long as this occurs, there may be delays in custom dashboards and Analyze Infrastructure for some TUs. ## Remediation Steps to take _until remediation_ - Verify that there are no issues with Kafka. - Consider increasing the number of tag-processor instances to keep up with increased load. You can go to an individual tag-processor dropiwizard dashboard to see which TUs are affected (check the `com.instana.tags.processing.SnapshotUpdateDropper.<tenant-unit>.error_rate` metrics) To get more context: [ðŸ”µ](https://blue-instanaops.instana.io/#/customDashboards/view;dashboardId=l6yRfqedQn-t_GqhGVHYmg) [ðŸŸ¢](https://green-instanaops.instana.io/#/customDashboards/view;dashboardId=9VDfDX1UQ7yAMuBj14CK7A) [ðŸ”´](https://red-instanaops.instana.io/#/customDashboards/view;dashboardId=jDwGyhY8SAq88FJXvtsqIg) [ðŸŸ ](https://orange-instanaops.instana.io/#/customDashboards/view;dashboardId=YalysPDKRoGpzuagPYtWwg)                                                                                   |
| [DEVSLO] tag-processor is dropping tag propagation actions [TF]                 | Tag processor is having trouble keeping up with tag changes and tag propagation for some TUs. ## Consequences As long as this occurs, there may be delays in custom dashboards and Analyze Infrastructure for some TUs. ## Remediation Steps to take _until remediation_ - Verify that there are no issues with Kafka or State Cassandra. - Consider increasing the number of tag-processor instances to keep up with increased load.  You can go to an individual tag-processor dropiwizard dashboard to see which TUs are affected (check the `com.instana.tags.processing.TagPropagationActionDropper.<tenant-unit>.error_rate` metrics)  To get more context: [ðŸ”µ](https://blue-instanaops.instana.io/#/customDashboards/view;dashboardId=l6yRfqedQn-t_GqhGVHYmg) [ðŸŸ¢](https://green-instanaops.instana.io/#/customDashboards/view;dashboardId=9VDfDX1UQ7yAMuBj14CK7A) [ðŸ”´](https://red-instanaops.instana.io/#/customDashboards/view;dashboardId=jDwGyhY8SAq88FJXvtsqIg) [ðŸŸ ](https://orange-instanaops.instana.io/#/customDashboards/view;dashboardId=YalysPDKRoGpzuagPYtWwg)                                                                                                                                                          |
| [SRESLO] tag-processor is dropping tag set updates [TF]                         | Tag processor is having trouble writing tag sets into Elasticsearch for some TUs. ## Consequences As long as this occurs, there may be delays in custom dashboards and Analyze Infrastructure for some TUs. ## Remediation Steps to take _until remediation_ - Verify that there are no issues with Kafka or Elasticsearch. - Consider increasing the number of tag-processor instances to keep up with increased load.  You can go to an individual tag-processor dropiwizard dashboard to see which TUs are affected (check the `com.instana.tags.processing.TagSetUpdateDropper.<tenant-unit>.error_rate` metrics)  To get more context: [ðŸ”µ](https://blue-instanaops.instana.io/#/customDashboards/view;dashboardId=l6yRfqedQn-t_GqhGVHYmg) [ðŸŸ¢](https://green-instanaops.instana.io/#/customDashboards/view;dashboardId=9VDfDX1UQ7yAMuBj14CK7A) [ðŸ”´](https://red-instanaops.instana.io/#/customDashboards/view;dashboardId=jDwGyhY8SAq88FJXvtsqIg) [ðŸŸ ](https://orange-instanaops.instana.io/#/customDashboards/view;dashboardId=YalysPDKRoGpzuagPYtWwg)                                                                                                                                                                                |
| [SRESLO] tag-processor is dropping tagged metrics [TF]                          | Tag processor is having trouble keeping up with the number of tagged metrics that are being published from some TUs. ## Consequences As long as this occurs, there may be delays in multi-dimensional metrics in custom dashboards and Analyze Infrastructure for some TUs. ## Remediation Steps to take _until remediation_ - Verify that there are no issues with Kafka. - Consider increasing the number of tag-processor instances to keep up with increased load. - Consider decreasing the filler tagged metric publish count  You can go to an individual tag-processor dropiwizard dashboard to see which TUs are affected (check the `com.instana.tags.processing.TaggedMetricDropper.<tenant-unit>.error_rate` metrics)  To get more context: [ðŸ”µ](https://blue-instanaops.instana.io/#/customDashboards/view;dashboardId=l6yRfqedQn-t_GqhGVHYmg) [ðŸŸ¢](https://green-instanaops.instana.io/#/customDashboards/view;dashboardId=9VDfDX1UQ7yAMuBj14CK7A) [ðŸ”´](https://red-instanaops.instana.io/#/customDashboards/view;dashboardId=jDwGyhY8SAq88FJXvtsqIg) [ðŸŸ ](https://orange-instanaops.instana.io/#/customDashboards/view;dashboardId=YalysPDKRoGpzuagPYtWwg)                                                                    |
| [SRESLO] tag-processor is lagging [TF]                                          | Updates written to ES are not live - we have some lag in some part of the tag-processor pipeline. This can come from: - latency writing into ES - latency in State Cassandra - latency writing to Kafka - High GC or High CPU from too many upstream updates ## Consequences There will be delays and possibly data-loss for changes that are occurring in customer environments in Custom Dashboards or Infrastructure Analyze. ## Remediation Steps to take _until remediation_ - Consider adjusting the dropping configuration e.g. decrease `config.tag.processor.start.dropping.age.ms` to enforce more dropping and reduce lag. - Verify that there are no issues with Kafka or Elasticsearch. - Consider increasing the number of tag-processor instances to keep up with increased load. To get more context: [ðŸ”µ](https://blue-instanaops.instana.io/#/customDashboards/view;dashboardId=l6yRfqedQn-t_GqhGVHYmg) [ðŸŸ¢](https://green-instanaops.instana.io/#/customDashboards/view;dashboardId=9VDfDX1UQ7yAMuBj14CK7A) [ðŸ”´](https://red-instanaops.instana.io/#/customDashboards/view;dashboardId=jDwGyhY8SAq88FJXvtsqIg) [ðŸŸ ](https://orange-instanaops.instana.io/#/customDashboards/view;dashboardId=YalysPDKRoGpzuagPYtWwg)      |
 
## Installation and Usage

With [Instana CLI for integration package management](https://github.com/instana/observability-as-code?tab=readme-ov-file#instana-cli-for-integration-package-management), you can manage the lifecycle of this package, such as downloading the package and importing it into Instana. You can find the available binaries for the CLI on different platforms on the [release page of this project](https://github.com/instana/observability-as-code/releases). Select the binary from the latest release that matches your platform to download, then rename it to stanctl-integration. You should now be able to run it on your local machine.

Downloading the package:

```shell
$ stanctl-integration download --package @instana-integration/instana-self-monitoring
```

Importing the package into Instana:

```shell
$ stanctl-integration import --package @instana-integration/instana-self-monitoring 
  --server $INSTANA_SERVER 
  --token $API_TOKEN 
  --set servicename=$SERVICE_NAME 
  --set serviceinstanceid=$SERVICE_INSTANCE_ID
```

- INSTANA_SERVER: This is the base URL of an Instana tenant unit, e.g. https://test-example.instana.io, which is used by the CLI to communicate with Instana server for package lifecycle management.
- API_TOKEN: Requests against the Instana API require valid API tokens. The API token can be generated via the Instana user interface. For more information, please refer to [Instana documentation](https://www.ibm.com/docs/en/instana-observability/current?topic=apis-instana-rest-api#usage-of-api-token).
- SERVICE_NAME: Logical name of the service.
- SERVICE_INSTANCE_ID: The string ID of the service instance. The ID helps to distinguish instances of the same service that exist at the same time (e.g. instances of a horizontally scaled service).